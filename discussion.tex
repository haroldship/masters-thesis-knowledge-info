% !TEX root = thesis.tex

%%
%%
%% Discussion chapter
%%
%%

%The contribution of this thesis is an empirical statistical analysis of the accuracy of the \gls{dkd}
%as a tool for estimating the incidence risk of chronic diseases.
In this research we studied how different factors of the population and incidence distributions
affect the \textit{accuracy} of the \gls{dkd}.
We examined the effect of several factors on the \gls{dkd} accuracy,
and we measured the accuracy of the \gls{dkd} using \gls{mise},
\gls{miae}, \gls{supremum error}, as well as \gls{peak bias},
\gls{peak drift} and \gls{centroid bias} and \gls{centroid drift},
as described in \Cref{sec:method:accuracy}.
We ran several scenarios to examine how variations in these factors which affect the incidence risk function,
the population distribution, and the sample size affect the accuracy of \gls{dkd} for estimation.
We compared two bandwidth selection techniques, \gls{silverman}'s rule of thumb,
and least-squares \glsentrylong{cv},
both optimized to minimize \gls{mise}.
We also compared these two techniques to an \gls{oracle},
which is an approximation of the theoretical optimal bandwidth,
keeping in mind that the oracle is only computable for simulations because we know the true function we wish to estimate.

Each scenario consisted of a set of experiments,
with each experiment consisting of 1,000 monte carlo simulations.
Each simulation was a random realization of the experimenatal setup.
In this way,
we obtained empirical estimates of each of the above accuracy measures for the \gls{dkd}
for each bandwidth selection technique.
However,
we found that the computations of each experiment required a lot of time,
and so we tried several methods to reduce this time.
We attempted to speed up the cross-validation bandwidth selection by using gradient descent.
We found the gradient descent algorithm often resulted in severe oversmoothing,
as the cross-validation error would decrease slowly as the bandwidth increased.
This required a lot of manual tuning of the learning rate parameter,
and so required re-running the experiment several times.
We added \textit{momentum} to our gradient descent implementation but it did not help in every case,
and so we changed our strategy to use parallelizaiton as described in \Cref{sec:method:experiment_structure}.

We observed that the estimation errors observed using the \gls{silverman} Rule of Thumb bandwidth selection method
were nearly as good as \gls{cv} in every case.
Since computing the \gls{silverman} bandwidth is much simpler than \gls{cv},
it would seem that using \gls{silverman} is warranted as a preliminary step such as for exploratory analysis.
However,
we note that in \Cref{tab:mean_error_rates:p0.7_100_1.0_1h} the accuracy measure \gls{mise} using the \gls{silverman} rule of thumb is even better than was obtained using the \gls{oracle}.
We tried to run with an additional experiment using 499 monte carlo simulations instead of 49 to compute the oracle bandwidth,
and found that in this case the \gls{silverman} rule did not outperform the \gls{oracle}.

We found that estimating the peak of the true function using the global maximum value can be improved upon.
Our method of using the 5\% centroid gave better results for both the magnitude and location of the peak most of the time.

Our results for those experiments which consist of different \gls{risk} functions on
\textbf{uniform populations},
that is,
for those for which the \gls{dkd} reduces to a kernel intensity estimate
have lower estimation errors than those which estimate
\gls{risk} functions on population distributions with a single peak.
This indicates that the population distribution has a noticeable negative effect on the accuracy of the \gls{dkd}.


