% !TEX root = thesis.tex

%%
%%
%% Discussion chapter
%%
%%

%The contribution of this thesis is an empirical statistical analysis of the accuracy of the \gls{dkd}
%as a tool for estimating the incidence risk of chronic diseases.
In this research we studied how different factors of the population and incidence distributions
affect the \textit{accuracy} of the \gls{dkd}.
We examined the effect of several factors on the \gls{dkd} accuracy,
and we measured the accuracy of the \gls{dkd} using \gls{mise},
\gls{miae}, \gls{supremum error}, as well as \gls{peak bias},
\gls{peak drift} and \gls{centroid bias} and \gls{centroid drift},
as described in \Cref{sec:method:accuracy}.
We ran several scenarios to examine how variations in these factors which affect the incidence risk function,
the population distribution, and the sample size affect the accuracy of \gls{dkd} for estimation.
We compared two bandwidth selection techniques, \gls{silverman}'s rule of thumb,
and least-squares \glsentrylong{cv}.
We also compared these two techniques to an \gls{oracle},
which is an approximation of the theoretical optimal bandwidth,
keeping in mind that the oracle is only computable for simulations because we know the true function we wish to estimate.

Each scenario consisted of a set of experiments,
with each experiment consisting of 1,000 monte carlo simulations.
Each simulation was a random realization of the experimenatal setup.
In this way,
we obtained empirical estimates of each of the above accuracy measures for the \gls{dkd}
for each bandwidth selection technique.
However,
we found that the computations of each experiment required a lot of time,
and so we tried several methods to reduce this time.
We attempted to speed up the cross-validation bandwidth selection by using gradient descent.
We found the gradient descent algorithm often resulted in severe oversmoothing,
as the cross-validation error would decrease slowly as the bandwidth increased.
This required a lot of manual tuning of the learning rate parameter,
and so required re-running the experiment several times.
We added \textit{momentum} to our gradient descent implementation but it did not help in every case,
and so we changed our strategy to use parallelizaiton as described in \Cref{sec:method:experiment_structure}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Section: Accuracy using Silverman
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Accuracy using Silverman}

In \Cref{tab:mean_error_rates:p0.7_100_1.0_1h} we see that the accuracy measure \gls{mise} using the \gls{silverman} rule of thumb is even better than was obtained using the \gls{oracle}.
We tried to run with an additional experiment with 499 monte carlo simulations, and found that in this case the \gls{silverman} rule did not outperform the \gls{oracle}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Section: Summary
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}

We used monte carlo simulations to empirically estimate the accuracy of the \acrlong{dkd} estimate of simulated disease risk intensity functions.
For this, we used several accuracy measures that measure the overall accuracy as well as the accuracy of the peak.
We also used two bandwidth selection methods to choose the bandwidth of the \gls{dkd}.
We did this while varying several factors.
In \Cref{sec:results:number_of_incidents} increasing the \gls{factor} resulted in decreased selected bandwidths
and increase relative accuracy, although absolute accuracy decreased.
Increasing the incidence \gls{spread} also resulted in increased relative accuracy.
Increasing the population size also resulted in increased accuracy,
while changing the population \gls{spread} resulted in unpredictable accuracy measures.
These results are interesting,
and there are are many opportunities for further work.


