% !TEX root = thesis.tex

%%
%%
%% Theoretical background chapter
%%
%%


The theoretical results in this chapter are taken from \citet{diggle1988equivalence}, \citet{guan2008consistent}, and \citet{wand1994kernel}.

\begin{defn}
``A \gls{spp} is any stochastic mechanism which generates a countable set of events in the plane.''
\end{defn}

Let \(N\) be a \gls{spp}, and let \(W \subset \mathbb{R}^2 \) be the study area.

The intensity \gls{lambda} at \(\vec{s} \in \mathbb{R}^2\) is defined as:

\begin{equation}
\label{eq:lambda}
    \gls{lambda} (\vec{s}) = \lim_{|\ds| \to 0}
        \left\{
            \frac{\mathbb{E}[N(\ds)]}% /
            {|\ds|}
        \right\}
\end{equation}

And \(\gls{lambda}(\vec{s})|\ds\) is the approximate probability of \ds~ containing a single incident.

Goal of intensity estimate: obtain an estimate of the function \(\lambda(\vec{s})\) on the study area by inference on a data sample.

Kernel intensity estimator (my simplification):
\begin{equation}
\label{eq:lambda_hat}
    \gls{lambda_hat}(\vec{s}; h) = \frac{1}{h} \sum_{\vec{x} \in N \cap W}{k((\vec{x}-\vec{s})/h)}
\end{equation}

where \gls{mu} is ... and \gls{k} is ...

%
% Convergence and inconsistency
%
\section{Inconsistency of the kernel intensity estimator}

From \citet{guan2008consistent}:

{
\color{red}
\textbf{The kernel intensity estimator is not consistent.}
This is because, although the kernel only uses local information around the point of interest to compute the estimate.
The resulting estimate has bias that tends to zero.
However it's variance does not diminish because the number of events in any fixed region is of order 1.
}

The paper develops a \textit{new nonparametric intensity estimator} that is consistent under ``some suitable yet reasonable conditions.''

%
% Bias and variance
%
\section{Bias and variance of kernel intensity estimator}

If we let \(\gls{lambda} = \gls{mu} f\) where \(f\) is a probability density, then at a point \(\vec{x} \in \mathbb{R}^2\),
\begin{align}
\mbox{bias}(\gls{lambda_hat}(\vec{x};h)) & \approx \frac{1}{2} \mu h^2 \gls{grad}^2~\left[ f(\vec{x}) \right] \int {z^2 K(\vec{z}) \diff \vec{z}} \\
\mbox{var}(\gls{lambda_hat}(\vec{x};h)) & \approx h^{-2} \mu \int {K(\vec{z})^2\diff \vec{z}} f(\vec{x})
\end{align}

Taking derivative by \(h\) and setting to \(0\), we get that \(h_{\mbox{opt}}\) is \(O(\mu^{-1/6})\).

Also, as \(\mu \to \infty\), both the bias and variance also \(\to \infty\),
but the \textit{relative} MISE, taken by dividing by \(\mu^2\) results in the relative bias being \(O(\mu^{-1/3})\)
but the relative variance being \(O(\mu^{1/3})\).



