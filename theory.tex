% !TEX root = thesis.tex

%%
%%
%% Theoretical background chapter
%%
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Disease incidence
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Disease incidence}
\label{sec:theory:incidence}

The theory described in this section is based on \citet{rothman2008modern}.
We focus on chronic diseases in order to better understand the degree to which different groups of people are ``at risk'' of contracting a disease.
We also are interested in discovering and describing the association between environmental factors and this ``risk''.
For example, one could ask if air pollution emitted by a factory is associated with an increased ``risk'' of lung cancer among people that live close by.

In order to be more precise,
let us define

\begin{defn}
    \textbf{\Gls{risk}} is the probability of an individual person contracting a disease in a specific time period.
\end{defn}

One way to estimate the \gls{risk} is to look at the historical frequency of contracting of the disease.
The type of study we are interested in involves incidents of a chronic disease.
Therefore, we separately consider those people who contracted the disease under study.
In particular, we are interested in people who did not have the disease at the beginning of the study,
but developed the disease within the study period.
These are new cases of the disease, which we call \textbf{\glspl{incident}}.

\begin{defn}
    An \textbf{\gls{incident}} is a new case of the disease during the study period.
\end{defn}

The common measure of frequency used in the literature compares the number of new cases of a disease to the total time spent by members of the population in the study.
This measure is called the \gls{incidence rate}.

\begin{equation}
    \mbox{\Gls{incidence rate} (common definition)} = \frac{\mbox{number of new cases}}
                                {\displaystyle\smashoperator{\sum\limits_{\mbox{persons}}}\mbox{time spent by person in population}}
\end{equation}

For our simulation study, we can simplify our definition of \gls{incidence rate} by assuming that each person in the population is exposed for the entire study period,
which for simplicity we can set to one (1).
Our simplified definition of incidence rate is then:

\begin{defn}
    The \textbf{\gls{incidence rate}} is the number of new cases in the study period divided by the total population.
    \begin{equation}
        \label{eq:incidencerate}
        \mbox{\Gls{incidence rate}} = \frac{\mbox{number of new cases in a time period}}
                                        {\mbox{number of persons}}
    \end{equation}
\end{defn}

\Glspl{incidence rate} are always expressed in cases per person-time units.
For example, 10 cases of asthma per 100,000 people per year is a valid \gls{incidence rate}.

For a given population, time period, and set of \glspl{incident},
we can compute the overall \gls{incidence rate} by taking the total number of \glspl{incident} and dividing by the total population.
However, we may wish to compare the \gls{incidence rate} at different locations within an area.
To do this, we need to look at the number of cases and the population in the areas that are close to these locations.
If we then make these areas infinitesimally small,
then we could compute the \gls{incidence rate} point-wise.

\begin{defn}
    \label{defn:incidence_rate}
    The \textbf{point-wise \gls{incidence rate}} is a function that computes the local \gls{incidence rate} at a point by dividing the point-wise value of the number of new cases by the point-wise value of the population.
\end{defn}

From this point onwards, the term \gls{incidence rate} refers to the point-wise \gls{incidence rate}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Spatial point processes
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Spatial point processes}
\label{sec:theory:spatial_point_processes}

The theoretical results in this section are taken from \citet{diggle1983spatial},
\citet{diggle1988equivalence},
\citet{guan2008consistent},
\citet{silverman1986density},
and \citet{wand1994kernel}.

In order to understand the geospatial patterns of disease incidence,
we consider points in two-dimensional space $\RS$.
We begin with

\begin{defn}
    \label{defn:studyarea}
    A \textbf{study area} $\gls{W} \subset \RS$ is a region in the plane where we make observations.
\end{defn}

The observations we are interested in are points distributed in \gls{W} which we call \textbf{events}.

\begin{defn}
    \label{defn:event}
    An \textbf{event} is a point $\xvec \in \RS$, situated in a study area \gls{W} that we are interested in observing.
\end{defn}

For example, in the study area of the boundaries of a city, we consider the events of the addresses of the residents.
We call this set of events

\begin{defn}
    The \textbf{population} \gls{P} is the set of home address events of the residents of the study area \gls{W}.
\end{defn}

In general, we are interested in understanding the mechanism by which the events occurred.
We are interested both in the number of events, and even more so in the pattern by which their locations are distributed in \gls{W}.
Taking a spatial statistical approach, we model the underlying mechanism that generates events using the following:

\begin{defn}
    \label{defn:spp}
    A \textbf{\gls{spp}} $\boldsymbol{\gls{Lambda}}$ is a stochastic mechanism which generates a random set of events.
\end{defn}

Suppose we have a \gls{spp} \gls{Lambda} on a study area $\gls{W} \subset \RS$.
We can count the total number of \glspl{event} and divide it by the total area of \gls{W}.
This gives us a quantity we call the \textbf{average \gls{intensity}} of \gls{Lambda}.
Sometimes the points are spread uniformly throughout \gls{W}.
In that case, we say that \gls{Lambda} is a homogeneous point process.
On the other hand, it is often the case that \gls{Lambda} is not uniform,
and we call it a heterogeneous point process.
When \gls{Lambda} is heterogeneous,
we would like to know the average \gls{intensity} around different points in \gls{W}.
For this
we define $\ds$ as an infinitesimal area around the point $\xvec$,
and $N\!(\ds)$ as the number of \glspl{event} inside $\ds$.
Then the \textbf{\gls{intensity}} $\boldsymbol{\gls{lambda}\xvec}$ at $\xvec \in \RS$ is

\begin{equation}
    \label{eq:lambda_differential}
    \lambda \xvec := \lim_{|\ds| \to 0}
        \left\{
            \frac{\mathbb{E}[N\!(\ds)]}% /
            {|\ds|}
        \right\}
\end{equation}

We model disease incidence as a special kind of \gls{spp} known as an Inhomogeneous Poisson Process.
\begin{defn}
    \label{defn:inhomogenouspoissonprocess}
    An \textbf{Inhomogeneous Poisson Process} is a \gls{spp} which has the following properties \citep[Section 4.4]{diggle1983spatial}:
    \begin{enumerate}
        \item The number of events $N\!(A)$ in any area $A$ has a Poisson distribution with mean
            $\iint\limits_{\xvec \in A}{\hspace{-1em} \lambda \xvec \diff{x_2} \diff{x_1}}$
            \label{defn:inhomogenouspoissonprocess:1}.
        \item With the condition that $N\!(A) \! = \! n$,
            the $n$ events (locations) in $A$ form an independent random sample from the distribution on $A$ with probability density function \textit{proportional} to $\lambda$ \label{defn:inhomogenouspoissonprocess:2}.
    \end{enumerate}
\end{defn}

From above, we can see that for an infinitesimal area \ds around a point \xvec,
we can think of $\lambda \xvec \ds$ as nearly constant in \ds~so
$$\iint\limits_{\xvec \in A}{\hspace{-1em}\lambda \xvec \diff{x_2} \diff{x_1}} \approx \lambda \xvec \ds \text{.}$$
By \Cref{defn:inhomogenouspoissonprocess:1} of \Cref{defn:inhomogenouspoissonprocess},
the probability that the are \ds~ contains exactly one point is
$\exp(-\lambda \xvec |\!\ds|) \lambda \xvec \ds \text{.}$
Since we take \ds~ to be infinitesimal,
$\exp(-\lambda \xvec |\!\ds|) \allowbreak \approx 1$ and so we can think of
$\lambda \xvec \ds$ as the probability that \ds~ contains exactly one event.

Qualitatively,
an intensity function differs from a probability density function in that it does not necessarily integrate to $1$.
In particular, we can write $\gls{lambda}$ as a scaled probability density function
where $f$ is a probability density function and \gls{mu} is a constant.

\begin{equation}
    \label{eq:lambda_mu}
    \gls{lambda}\xvec = \mu f\!\xvec
\end{equation}

For a point-wise \gls{incidence rate} function,
we compute two separate \gls{intensity} functions.
The first \gls{intensity} returns the point-wise expected number of cases of the disease,
while the second returns the point-wise population density.
Dividing the value obtained from first by the value obtained from the second results in the \gls{incidence rate} described in \Cref{defn:incidence_rate}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Kernel estimation of the intensity
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kernel estimation of the intensity}
\label{sec:theory:kernelestimation}

This section is based on \citet{silverman1986density} and \citet{wand1994kernel}.
The usual case in practice is to start with a set of \glspl{event} that have been observed in a study area.
For example, we can have new cases of a disease in a city that were diagnosed over the course of a defined time period.
By our \Cref{defn:event},
we consider the location coordinates such as those of the home addresses of the people who were diagnosed.
Our goal is to estimate the \gls{intensity} function for disease cases using these observed \glspl{event}.
Similarly for the population,
we would like to estimate the population \gls{intensity} function.

A common method for estimating \gls{intensity} functions is known as
\textbf{\gls{kernel intensity estimation}}.
This technique utilizes a special function known as a \textbf{\gls{kernel}},
denoted by $\gls{K}\xvec$,
that satisfies
\begin{align}
    & \hspace{1.75em} \gls{K}\xvec \ge 0~ \forall \xvec \in W, \text{and} \label{eq:k_pos} \\%
    & \iintW{\gls{K}\xvec} = 1 \label{eq:k_1} \text{.}%
\end{align}

\Cref{eq:k_pos,eq:k_1} are equivalent to stating that \Kdots~is a probability density on $\RS$.

Spatial analysis uses a two-dimensional version of \gls{kernel intensity estimator}
which is a natural extension to $\RS$ of the one dimensional \gls{kernel intensity estimator} $\gls{lambda_hat}_{1,h}$.
\begin{equation}
    \label{eq:lambda_hat_1}
    \gls{lambda_hat}_{1,h} = \sumSone{K_1\left( \frac{x - s}{h} \right)} \text{.}%
\end{equation}
Here, $K_1(x)$ is a one-dimensional kernel function.

In order to compute $\gls{lambda_hat}_{1,h}$ using \Cref{eq:lambda_hat_1},
there are two choices to be made: the choice of the bandwidth \gls{h}
and the choice of \gls{kernel} $K_1(\cdot)$.
The role of the \gls{kernel} function $K_1(\cdot)$~is to smooth the information obtained
from the observed \glspl{event} to an area around their locations.
Choosing different \glspl{kernel} results in different shapes to this smoothing.
However, this has only a small effect on the resulting \gls{intensity} estimate.
On the other hand,
the choice of bandwidth \gls{h} has a major effect on the \gls{intensity} estimate.
The bandwidth controls the width of the area smoothed by the \gls{kernel} function and hence on the overall amount of smoothing.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{img/kernel1d-02}
        \subcaption{$h=0.2$}
        \label{fig:theory:kernel1d:02}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{img/kernel1d-04}
        \subcaption{$h=0.4$}
        \label{fig:theory:kernel1d:04}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{img/kernel1d-08}
        \subcaption{$h=0.8$}
        \label{fig:theory:kernel1d:08}
    \end{subfigure}
    \caption[One-dimensional \glsentryname{kernel} functions]
        {One-dimensional \glsentryname{kernel} functions using the biweight(quartic) kernel \citep{silverman1986density}.
        The dots on the $x_1$ axis are the observed \glsentryplural{event}.
        The dashed lines are the kernel functions around these \glsentryplural{event}
        and the solid line is the \glsentryname{intensity} estimate $\hat{\lambda_1}$.}
    \label{fig:theory:kernel1d}
\end{figure}

\Cref{fig:theory:kernel1d} shows an example of how \gls{kernel} smoothing works in  one-dimension for three different bandwidths.
The dots along the bottom are the observed \glspl{event}.
The dashed lines show the \gls{kernel} functions around each of these \glspl{event},
while the upper solid line is the \gls{intensity} estimate produced using \Cref{eq:lambda_hat_1}.
\Cref{fig:theory:kernel1d:02} shows the estimate with $h=0.2$.
It has two local modes, each with a bit of fluctuation at the peaks.
In \Cref{fig:theory:kernel1d:04} we still see two modes with $h=0.4$, but with smoother peaks.
\Cref{fig:theory:kernel1d:08} has smoothed the two peaks together into a single,
large peak by using the larger bandwidth of $h=0.8$.

The two-dimensional case is analogous.
Starting with a set of \glspl{event} $\gls{N} \subset \RS$,
and a positive value \gls{h} known as the bandwidth,
we define the the \gls{kernel intensity estimator} as follows:
\begin{defn}
    \label{defn:lambda_hat}
    The \textbf{\gls{kernel intensity estimator}} $\gls{lambda_hat}_h \xvec$
    is a technique for estimating the \gls{intensity} of an \gls{spp}
    from observed \glspl{event} \gls{N}.
    The formula for computing the \gls{kernel intensity estimator} is:
    \begin{equation}
        \label{eq:lambda_hat}
        \gls{lambda_hat}_h \xvec%
            = \sumS{\gls{K}\left( \frac{\xvec-(s_1, s_2)}{h} \right)} \text{.}%
    \end{equation}
\end{defn}
where $\xvec-(s_1, s_2)$ is ordinary vector subtraction,
and $\frac{\xvec-(s_1, s_2)}{h}$ is scalar multiplication by $\frac{1}{h}$.
For the remainder of this section we treat $h$ as a scalar for ease of explanation.
However, with no loss of generality we will use separate bandwidths in each coordinate,
that is $\mathbf{h}=(h_1, h_2)$.

There are many kernels which satisfy \Cref{eq:k_pos,eq:k_1},
including two-dimensional versions of the square kernel, the Gaussian kernel and the Epanechnikov kernel.
In this study we use the two-dimensional biweight kernel,
also known as the quartic kernel,
which can be found in \citet{silverman1986density} with formula
\begin{equation}
    \label{eq:biweightkernel2d}
    K \xvec =%
    \begin{cases}%
        \frac{3}{\pi}\left( 1 - x_1^2 - x_2^2 \right)^2 & \text{if}~x_1^2+x_2^2<1 \\%
        0 & \text{otherwise.}%
    \end{cases}%
\end{equation}
This kernel spreads the weight of each event \xvec~ into a circle of radius one, centered at \xvec.
The effect of the bandwidth $h$ is to change the radius of that circle to $h$.
The advantage of the kernels over the Epanechnikov kernel is that it twice-differentiable.
The advantage over the Gaussian kernel is that it much faster to compute.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Measuring the accuracy of the estimate
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measuring the accuracy of the estimate}
\label{sec:theory:accuracy}

This section is based on \citet{silverman1986density} and \citet{wand1994kernel}.
A common statistical problem is to find the ``best'' estimate \gls{lambda_hat}
for the \gls{intensity} of an unknown \gls{spp}
based on a set of observed \glspl{event}.
In order to do this using \gls{kernel intensity estimation},
we must select the ``best'' bandwidth.
From \Cref{fig:theory:kernel1d} we can see that different values of bandwidth \gls{h} result in different estimates for the \gls{intensity}.
Our goal is to find a decision procedure that gives us the ``best'' result.
While there is a great deal of research in \gls{kde},
there is much less work available for \gls{kernel intensity estimation}.
We make use of one such result:
the optimal bandwidth used for computing the \gls{kernel intensity estimator} is the same as that for computing the related \gls{kde} function,
and can be determined by the same techniques \Citep{diggle1988equivalence}.
However, the \gls{kde} is used to estimate a probability density function,
while the \gls{kernel intensity estimator} is used to compute an intensity function.
Computationally, the difference is that a density must integrate to one,
and in order to do that,
we divide by $n$,
the number of observed events.


The first thing we must do is define what we mean by the ``best'' result.
We do this by choosing a measure of accuracy for the estimate,
and selecting the bandwidth which gives the optimal value for this measure.
One common accuracy measure used for function estimation is the \gls{mise},
because it has several useful mathematical properties.
It allows for additional analysis by being broken down into the mean integrated squared bias and mean integrated variance.
It can also be estimated by \acrlong{cv}.

Given a realized estimate \gls{lambda_hat} for any function \gls{lambda},
we can compute the squared error at any given point $\xvec \in \RS$ as
$\left( \lmbdhat - \lmbd \right)^2$,
and its expected value at $\xvec$ is called the \gls{mse}.
However,
we would like to compute the accuracy of the whole function.
We do this by integrating the squared error of the estimate over the study area to obtain the \gls{ise}.
\begin{equation}
\label{eq:ise}
    \mbox{\gls{ise}}(\gls{lambda_hat}) = \iintW{
            \left( \lmbdhat - \lmbd \right)^2
    }
\end{equation}
The \gls{mise} is the expected value of the \gls{ise} over the sample space of realizations of \gls{Lambda}.
\begin{align}
    \mbox{\gls{mise}}(\gls{lambda_hat}) 
        & = \E [\mbox{\gls{ise}}(\gls{lambda_hat})] \nonumber \\
        & = \E \iintW{ \left( \lmbdhat - \lmbd \right)^2 } \label{eq:mise}
\end{align}
The integrand of \Cref{eq:mise} is non-negative.
Therefore, by Fubini's Theorem, we can change the order of integration and expectation.
\begin{align}
    \mbox{\gls{mise}}(\gls{lambda_hat}) 
        & = \iintW{ \E \left( \lmbdhat - \lmbd \right)^2 } \nonumber \\
        & = \iintW{ \mbox{\gls{mse}}(\gls{lambda_hat}, \xvec) } \nonumber
\end{align}

We decompose \gls{mse} into the squared bias and squared variance% (\Cref{eq:mse:biasvariance}),
giving us
\begin{align}
    \mbox{\gls{mise}}(\gls{lambda_hat}) 
        & = \iintW{ \Big( \E\lmbdhat - \lmbd \Big)^2 } \nonumber \\
        & \qquad + \iintW{ \E \Big( \lmbdhat - \E\lmbdhat \Big)^2 } \label{eq:mise:intbiasvariance}
\end{align}
We call the first part of \Cref{eq:mise:intbiasvariance} the \gls{isb} and the second part the \gls{iv}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Asymptotic behavior of the bandwidth
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic behavior of the bandwidth}
\label{sec:theory:asymptotic_bandwidth}

For the \gls{kernel intensity estimator},
the \gls{mise} cannot be computed directly.
Instead,
we need to approximate it.
In order to do this,
we compute approximate values of the bias,
the variance,
and finally we can compute the \gls{amise} which is the sum of the approximate variance and the square of the approximate bias.
This allows us to analyze the behavior of the \gls{mise} asymptotically,
with dependence only on the true function $\gls{lambda}\dotdot$ and the kernel $K\dotdot$.

Following \citep{silverman1986density,wand1994kernel},
and based on the equivalence of bandwidths for the \gls{kde} and the\gls{kernel intensity estimator} \citep{diggle1988equivalence},
we begin by using Taylor's Theorem to approximate the \gls{kde}.
In order to do this we make the following assumptions about $K\dotdot$
and $f\dotdot$ \citep{silverman1986density,wand1994kernel}:
\begin{enumerate}
    \item $K\dotdot$ is a probability density function with zero mean \label{amise:assumptions:1}
    \item $K\dotdot$ is radially symmetric and has non-zero, finite variance \label{amise:assumptions:2}
    \item $f\dotdot$ has bounded and continuous second derivatives. \label{amise:assumptions:3}
\end{enumerate}

Under these assumptions,
the first term of the Taylor expansion is zero (\cref{amise:assumptions:1,amise:assumptions:3}),
and the second and remainder terms of the Taylor expansion are finite (\cref{amise:assumptions:2}).
The following formulas for the \gls{amise} can be found it \citet[Section 4.3]{silverman1986density}.
\begin{align}
    \mbox{bias}_h \hat{f} \xvec%
        & \approx \frac{1}{2} h^{2} \alpha \nabla^2 f \xvec \nonumber \\%
    \mbox{variance}_h \hat{f} \xvec%
        & \approx n^{-1} h^{-2} \beta f \xvec \nonumber \\%
    \mbox{\gls{amise}}(f; h)%
        & = \frac{1}{4} h^{4} \alpha^2 \iintW{\left( \nabla^2 f \xvec \right)^2 }%
            + n^{-1} h^{-2} \beta \label{eq:amisef}%
\end{align}
where
\begin{align}
    \alpha & = \iintR{x_1^2 K(x_1,x_2)} \text{, since}~K\dotdot~\text{is radially symmetric,} \nonumber \\
    \beta & = \iintR{\Big( K(x_1,x_2) \Big)^2} \text{, and} \nonumber \\
    \nabla^2 f \xvec & = \frac{\diffsq{f}\xvec}{\diff{x_1^2}} + \frac{\diffsq{f}\xvec}{\diff{x_2^2}} \nonumber \text{.}
\end{align}

We note that both $\alpha$ and $\beta$ depend only on \Kdots,
and that $\nabla^2 f$ is the Laplacian operator which is the trace of the Hessian matrix.
We can now take the derivative of \Cref{eq:amisef} by $h$ and set it to zero,
giving us the optimal bandwidth \gls{h_opt}:
\begin{align}
    \gls{h_opt} & = C n^{-\frac{1}{6}}
\end{align}
where $C$ is a constant that depends on $\lambda$ and $K$.
\begin{defn}
    The \gls{mise} \textbf{optimal bandwidth} $\mathbf{\gls{h_opt}}$ is the value of $h$ that minimizes the \gls{mise} of the \gls{kde} for a given probability density function $f$ and kernel function $K$.
\end{defn}
As noted above,
the optimal bandwidth of the \gls{kde} in the \gls{mise} sense is the same for the \gls{kernel intensity estimator}.
For $h < \gls{h_opt}$, the variance will be higher, while $h > \gls{h_opt}$ will result in a higher bias \Citep{rosenblatt1971curve}.

Since we have $\gls{lambda_hat} = n \hat{f}$,
the formula for the \gls{amise} of $\gls{lambda} = \mu f$ is
\begin{align}
    \mbox{bias}_h \gls{lambda_hat} \xvec%
        & \approx \frac{1}{2} h^{2} \alpha \mu \nabla^2 f \xvec \nonumber \\%
    \mbox{variance}_h  \gls{lambda_hat} \xvec%
        & \approx n^{-1} h^{-2} \beta \mu f \xvec \nonumber \\%
    \mbox{\gls{amise}}(\gls{lambda}; h)%
        & = \frac{1}{4} h^{4} \alpha^2 \mu^2 \iintW{\left( \nabla^2 f \xvec \right)^2 }%
            + \mu^2 n^{-1} h^{-2} \beta \label{eq:amiselambda}%
\end{align}

There are other accuracy measures that can be used to evaluate the estimate of the \gls{kernel intensity estimator}.
We discuss some of these in \Cref{sec:method:accuracy}.
Although we use these measures to characterize the behavior of the \gls{kernel intensity estimator} and the \gls{dkd},
our bandwidth is chosen by techniques that try to achieve \gls{h_opt} and minimize \gls{mise}.
However, as noted above,
our approximation \gls{amise} depends on the true risk function \gls{lambda}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Bandwidth selection from data
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bandwidth selection from data}
\label{sec:theory:bandwidthselection}

In practice,
researchers typically do not know the true density or intensity functions.
Therefore they cannot compute \gls{h_opt} directly,
and so they use techniques that give them approximations for this value.
In this study,
we do know the true intensity functions,
since we use them to generate data for the simulations.
Therefore we can compute the ``best'' bandwidth by comparing to the actual truth.
We therefore define
\begin{defn}
    the \textbf{\gls{oracle bandwidth}} for a given \gls{intensity} is the bandwidth that minimizes the empirical \gls{mise},
    computed from the data samples generated from the true \gls{intensity} function.
\end{defn}
Other schemes for selecting the bandwidth that are based on a specific data sample will not be as good as the \gls{oracle}.
The two techniques we use are optimized to find a bandwidth that minimizes \gls{mise}.
The selected bandwidth may or may not be close to \gls{h_opt},
depending on the particular sample.
The first technique we will use is known as \textbf{\Gls{silverman}'s Rule of Thumb}.
We use the version for selecting bandwidth in $\RS$,
which has forumula
\begin{equation}
    \label{eq:silverman}
    h_S = 2.04 * \sigma * n^{-1/6}%
\end{equation}
where $n$ is the observed number of incidents,
$\sigma$ the arithmetic mean of $\sigma_{x_1}$ and $\sigma_{x_2}$,
he standard deviations of the $x_1$ and $x_2$ coordinates of the sample points respectively:
\begin{equation*}
    \sigma = \frac{\sigma_{x_1} + \sigma_{x_1}}{2} \text{.}
\end{equation*}
\citet{silverman1986density} shows that for a large class of intensity functions,
including ones based on Gaussian functions,
this bandwidth provides a good approximation to the optimal bandwidth \gls{h_opt}. 

Our second technique is to use leave-one-out \textbf{\gls{cv}} over a set of bandwidths.
We select the bandwidth which minimizes the \gls{cv} error,
which approximates the \gls{mise} (\Cref{eq:mise}).
For the \gls{kde},
he formula for the \gls{cv} error of bandwidth $h$ for \gls{lambda_hat} is
\begin{equation}
    \label{eq:cverror}
    \mbox{\Gls{cv}}%
        = \iintW{\gls{lambda_hat}_h \xvec^2}
        - 2 n^{-1} \sum\limits_{i=1}^n \gls{lambda_hat}_{-i,h}\xvec%
\end{equation}
where $\gls{lambda_hat}_{-i,h}\xvec$ is the \gls{kernel intensity estimator} computed using the bandwidth $h$ and leaving out the $i^\text{th}$ point.
The technique for \gls{kernel intensity estimation} is identical to the \gls{kde},
in the sense that the bandwidths which minimize the \gls{mise} of the \gls{kernel intensity estimator} are the same \citep{brooks1991asymptotic}. 
\Citet{brooks1991asymptotic} show that the \gls{cv} error computed with the selected bandwidth approximates the optimal \gls{ise} very well,
in the sense that
\begin{equation*}
    \frac   {\mbox{\gls{ise}}(\gls{lambda_hat}; h_{\gls{cv}})}%
            {\mbox{\gls{ise}}(\gls{lambda_hat}; \gls{h_opt})}%
            \to 1 ~\text{almost surely.}%
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Inconsistency of the kernel intensity estimator
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inconsistency of the kernel intensity estimator}
\label{sec:theory:inconsistency}

The \gls{kde} $\hat{f}$ can be shown to be a consistent estimator of a density function $f$.
\citet{bertrand1978convergence} show that the \gls{kde} converges uniformly to the true function with probability 1 as $n \to \infty$,
while \citet{devroye1985nonparametric} show that the \gls{kde} converges in $L_1$ to the true function with probability 1.
In contrast to density a density function,
where the estimate is normalized by the number of observations,
for a spatial intensity
both the magnitude of the scaling factor \gls{mu} of the true intensity
and the number of observed events $n$ are proportional
to both the time frame of the study or the size of the study area.
Therefore,
in order to discuss the asymptotic behavior of an intensity function,
it makes sense to look at $\gls{mu} \to \infty$.
We describe the behaviour of the \gls{dkd} as \gls{mu} increases
can be found in in \Cref{sec:results:unifNpop_1h}.
Along these lines,
\citet{guan2008consistent} note that the \gls{kernel intensity estimator}
``is not a consistent estimator of the true intensity''.

We therefore \textit{normalize} our measures of accuracy in two ways.
The first way is to simply divide our approximated \gls{mise} by $\gls{mu}^2$
as suggested by \citet{diggle1988equivalence},
since we know the true function and therefore \gls{mu}.
The second way is to divide, at every point,
the estimation error $\gls{lambda_hat}\xvec - \gls{lambda}\xvec$ by the true value $\gls{lambda}\xvec$.
Our results contain these and similar relative errors for all of the accuracy measures
as described in \Cref{sec:method:accuracy}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Data generation process
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data generation process}
\label{sec:theory:data}

We generate data for each experiment using rejection sampling,
using the technique suggested in \Citet[Section 4.4]{diggle1983spatial},
based on \citet{lewis1979simulation}.
Our algorithm can be divided into two parts.
First, we generate a population \gls{P} as follows:
\begin{enumerate}
    \item create the true population intensity function \gls{lambda_P}
        by scaling a function of the desired shape
        so that we generate the desired population count $N_P$
    \item generate events uniformly in the study area \gls{W} (excluding the buffer)
    \item keep each point \xvec with probability \gls{lambda_P} \xvec
    \item repeat until we have $N_P$ points.
\end{enumerate}
Due to the large size of the population,
we use the same population \gls{P} for an entire experiment
in order to perform the experiments in a reasonable amount of time.

Once we have a population,
we can repeatedly simulate sample sets of incidents using the following procedure:
\begin{enumerate}
    \item create the incident intensity function \gls{lambda_I}
        by scaling a function of the desired shape
        so that the \textit{expected} number of incidents generated will be $N_I$.
        As will be seen,
        in order to do this we must take into account both $N_I$ and $N_P$.
    \item for each ``person'' $p\!=\!(p_1, p_2) \in \gls{P}$,
        calculate the \textit{risk} of coming down with the disease $\gls{lambda_I}(p_1, p_2)$
    \item keep $(p_1, p_2)$ in \gls{I} with probability $\gls{lambda_I}(p_1, p_2)$.
\end{enumerate}


