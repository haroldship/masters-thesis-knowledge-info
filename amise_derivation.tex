% !TEX root = amise_derivation_only.tex

\section{Single-variable Asymptotic Mean Integrated Squared Error}

Derivation of the Asymptotic Mean Integrated Squared Error of the kernel density estimator, based on \citep{wand1994kernel}

Define the kernel density estimator.

\begin{align}
    \hat{f}(x;h) & = (nh)^{-1} \sum_{i=1}^{n}{K\{(x-X_i)/h\}} && \\
        & = n^{-1} \sum_{i=1}^n{K_h(x-X_i)} &&
\end{align}

where:

\begin{align}
K_h(x) & = h^{-1} K(x/h) &&
\end{align}

\subsection{Mean Squared Error}

For a statistical parameter $\theta$, the mean squared error (MSE) of an estimator $\hat{\theta}$ is an indicator of how well the estimator approximates the true parameter.
It can be broken down into the sum of the variance and the squared bias.

\begin{align}
    \mbox{MSE}(\hat{\theta}) & =  E(\hat{\theta} - \theta)^2 && \nonumber \\
                & = E\theta^2 - 2\theta E\hat{\theta} + \theta^2 && \nonumber \\
                & = [E\theta^2 - (E\hat{\theta})^2] + [(E\hat{\theta})^2 - 2\theta E\hat{\theta} + \theta^2] && \nonumber \\
                & = [E\theta^2 - (E\hat{\theta})^2] + [(E\hat{\theta}) - \theta]^2 && \nonumber \\
                & = \mbox{Var}(\hat{\theta}) + \mbox{Bias}(\hat{\theta})^2
\end{align}

For the kernel density estimator at a given point $x$,

\begin{align}
    E\hat{f}(x;h) & = E K_h(x-X) && \nonumber \\
        & = \int{K_h(x-y)f(y)\,\mathrm{d}y} && \nonumber \\
        & = \conv{K_h}{f}(x)
\end{align}

where $X$ is a random variable having density function $f$.

\subsubsection{Bias of the estimator}

\begin{align}
\mbox{Bias}(\hat{f}(x;h)) & = E \hat{f}(x;h) - f(x) && \nonumber \\
    & = \conv{K_h}{f}(x) - f(x) &&
\end{align}

\subsubsection{Variance of the estimator}

\begin{align}
\mbox{Var}(\hat{f}(x;h) & = E\hat{f}(x;h))^2 - [E\hat{f}(x;h)]^2 && \nonumber \\
    & = E[n^{-1}\sum{K_h(x - X_i)}]^2 - [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} E K_h(x - X)^2 + n^{-2} E \sum_{i \neq j}{K_h(x-X_i)K_h(x-X_j)} - [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} \conv{K_h^2}{f}(x) + n(n - 1) / n^2 E K_h(x - X)^2 - n^2 / n^2 [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} \conv{K_h^2}{f}(x) - n^{-1} [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} [\conv{K_h^2}{f}(x) - \conv{K_h}{f}]^2(x)]
\end{align}

\subsubsection{MSE at a point}

At a given point $x$, the variance of the kernel density estimator is:

\begin{align}
\mbox{MSE}(\hat{f}(x;h)) & = \mbox{Var}(\hat{f}(x;h)) + \mbox{Bias}^2(\hat{f}(x;h)) && \nonumber \\
    & = n^{-1} [\conv{K_h^2}{f}(x) - \conv{K_h}{f}]^2(x)] + [\conv{K_h}{f}(x) - f(x)]^2 \label{eq:MSE}
\end{align}

\subsection{Mean Integrated Squared Error}

MSE gives an evaluation of the estimator at one point.
However, it is usually preferable to evaluate the estimator over the whole real line.
For this, we use the \textit{integrated} squared error (ISE).
In particular, we calculed the squared $\mathfrak{L}_2$ distance of the estimator $\hat{f}$ from the true function value at that point.

\begin{align}
\mbox{ISE}(\hat{f}(.;h)) & = \int{\{ \hat{f}(x;h) - f(x) \} ^2 }\,\mathrm{d}x && \nonumber
\end{align}

Since this is a random quantity, dependent upon the given sample, we prefer to compute its expected value.
This quantity is known as the \textit{mean integrated squared error} or MISE.

\begin{align}
\mbox{MISE}(\hat{f}(.;h)) & = E \int{\{ \hat{f}(x;h) - f(x) \} ^2 }\,\mathrm{d}x && \nonumber \\
    & = \int{ E \{ \hat{f}(x;h) - f(x) \} ^2 }\,\mathrm{d}x && \nonumber \\
    & = \int{ \mbox{MSE}(\hat{f}(x;h))\, \mathrm{d}x} & \text{change the order of integration} & \nonumber \\
    & = n^{-1} \int{ [ \conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x) ]\,\mathrm{d}x } \nonumber \\
    & \qquad + \int{ [ \conv{K_h}{f}(x) - f(x) ]^2\,\mathrm{d}x} & \text{by \autoref{eq:MSE}} \nonumber
\end{align}

Simplify it.

\begin{align}
\mbox{MISE}(\hat{f}(.;h)) & = n^{-1} \int{ [ \conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x) ]\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ [ \conv{K_h}{f}(x) - f(x) ]^2\,\mathrm{d}x} && \nonumber \\
    & = n^{-1} \int{ \conv{K_h^2}{f}(x)\,\mathrm{d}x } - n^{-1} \int{\conv{K_h}{f}^2(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ \conv{K_h}{f}^2(x)\,\mathrm{d}x} - 2\int{\conv{K_h}{f}(x) f(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ f^2(x)\,\mathrm{d}x} && \nonumber \\
    & = n^{-1} \int{ \conv{K_h^2}{f}(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + (1 - n^{-1}) \int{\conv{K_h}{f}^2(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad - 2\int{\conv{K_h}{f}(x) f(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ f^2(x)\,\mathrm{d}x} \label{eq:MISE-1} &&
\end{align}

Looking at the first term of \autoref{eq:MISE-1}, and using a change of variables $t = (x - y) / h$, so that $\mathrm{d}x = h \mathrm{d}t$

\begin{align}
n^{-1} \int{ \conv{K_h^2}{f}(x)\,\mathrm{d}x } & = n^{-1} \int{ \int {K_h^2(x-y)f(y)\,\mathrm{d}y} \,\mathrm{d}x } && \nonumber \\
    & = n^{-1} \int{ \int {h^{-2} K^2( (x-y)/h )f(y)\,\mathrm{d}y} \,\mathrm{d}x } && \nonumber \\
    & = n^{-1} \int{ \int {h^{-1} K^2(t)f(y)\,\mathrm{d}y} \,\mathrm{d}t } && \nonumber \\
    & = n^{-1} \int{ h^{-1} K^2(t) \int {f(y)\,\mathrm{d}y} \,\mathrm{d}t } && \nonumber \\
    & = (nh)^{-1} \int{ K^2(t) \,\mathrm{d}t } \label{eq:MISE-2} && \text{since f is a density}
\end{align}

Substituting \autoref{eq:MISE-2} in \autoref{eq:MISE-1}, gives the following formula for MISE:

\begin{align}
\mbox{MISE}(\hat{f}(.;h)) & = (nh)^{-1} \int{ K^2(x) \,\mathrm{d}x } && \nonumber \\
    & \qquad + (1 - n^{-1}) \int{\conv{K_h}{f}^2(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad - 2\int{\conv{K_h}{f}(x) f(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ f^2(x)\,\mathrm{d}x} \label{eq:MISE} &&
\end{align}

\subsubsection{Asymptotic Mean Squared Error}

The equation we derived for MISE, \autoref{eq:MISE}, depends on the bandwidth in such a way as to make it difficult to compute.
In this section we will derive an approximation, using Taylor's Theorem, that is far easier to compute.

There are 3 assumptions that we make:

\begin{enumerate}
    \item The true density $f$ has a second derivative $f\prime\prime$ that is continous, square integrable, and ultimately monotone as described in (\citep{wand1994kernel}.
    \item There is a sequence of bandwidths, \mbox{$\{h_n : 1 < n < \infty\}$} such that each \mbox{$h_n > 0$, $\lim_{n \to \infty} h_n = 0$} and \mbox{$\lim_{n \to \infty} nh = \infty$}.
    \item $K$ is a bounded probability density with a finite fourth moment and symmetry about the origin.
\end{enumerate}

