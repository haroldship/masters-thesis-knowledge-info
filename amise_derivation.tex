% !TEX root = amise_derivation_only.tex

\section{Single-variable AMISE}

Derivation of the Asymptotic Mean Integrated Squared Error of the kernel density estimator, based on \citep{wand1994kernel}

Define the kernel density estimator.

\begin{align}
    \hat{f}(x;h) & = (nh)^{-1} \sum_{i=1}^{n}{K\{(x-X_i)/h\}} && \\
        & = n^{-1} \sum_{i=1}^n{K_h(x-X_i)} &&
\end{align}

where:

\begin{align}
K_h(x) & = h^{-1} K(x/h) &&
\end{align}

\subsection{Mean Squared Error}

For a statistical parameter $\theta$, the mean squared error (MSE) of an estimator $\hat{\theta}$ is an indicator of how well the estimator approximates the true parameter.
It can be broken down into the sum of the variance and the squared bias.

\begin{align}
    \mbox{MSE}(\hat{\theta}) & =  E(\hat{\theta} - \theta)^2 && \nonumber \\
                & = E\theta^2 - 2\theta E\hat{\theta} + \theta^2 && \nonumber \\
                & = [E\theta^2 - (E\hat{\theta})^2] + [(E\hat{\theta})^2 - 2\theta E\hat{\theta} + \theta^2] && \nonumber \\
                & = [E\theta^2 - (E\hat{\theta})^2] + [(E\hat{\theta}) - \theta]^2 && \nonumber \\
                & = \mbox{Var}(\hat{\theta}) + \mbox{Bias}(\hat{\theta})^2
\end{align}

For the kernel density estimator at a given point $x$,

\begin{align}
    E\hat{f}(x;h) & = E K_h(x-X) && \nonumber \\
        & = \int{K_h(x-y)f(y)\,\mathrm{d}y} && \nonumber \\
        & = \conv{K_h}{f}(x) \label{eq:E_f_hat} &&
\end{align}

where $X$ is a random variable having density function $f$.

\subsubsection{Bias of the estimator}

\begin{align}
\mbox{Bias}(\hat{f}(x;h)) & = E \hat{f}(x;h) - f(x) && \nonumber \\
    & = \conv{K_h}{f}(x) - f(x) &&
\end{align}

\subsubsection{Variance of the estimator}

\begin{align}
\mbox{Var}(\hat{f}(x;h) & = E\hat{f}(x;h))^2 - [E\hat{f}(x;h)]^2 && \nonumber \\
    & = E[n^{-1}\sum{K_h(x - X_i)}]^2 - [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} E K_h(x - X)^2 + n^{-2} E \sum_{i \neq j}{K_h(x-X_i)K_h(x-X_j)} - [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} \conv{K_h^2}{f}(x) + n(n - 1) / n^2 E K_h(x - X)^2 - n^2 / n^2 [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} \conv{K_h^2}{f}(x) - n^{-1} [\conv{K_h}{f}(x)]^2 && \nonumber \\
    & = n^{-1} [\conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x)] \label{eq:VarEst}
\end{align}

\subsubsection{MSE at a point}

At a given point $x$, the mean squared error of the kernel density estimator is:

\begin{align}
\mbox{MSE}(\hat{f}(x;h)) & = \mbox{Var}(\hat{f}(x;h)) + \mbox{Bias}^2(\hat{f}(x;h)) && \nonumber \\
    & = n^{-1} [\conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x)] + [\conv{K_h}{f}(x) - f(x)]^2 \label{eq:MSE}
\end{align}

\subsection{Mean Integrated Squared Error}

MSE gives an evaluation of the estimator at one point.
However, it is usually preferable to evaluate the estimator over the whole real line.
For this, we use the \textit{integrated} squared error (ISE).
In particular, we calculed the squared $\mathfrak{L}_2$ distance of the estimator $\hat{f}$ from the true function value at that point.

\begin{align}
\mbox{ISE}(\hat{f}(.;h)) & = \int{\{ \hat{f}(x;h) - f(x) \} ^2 }\,\mathrm{d}x && \nonumber
\end{align}

Since this is a random quantity, dependent upon the given sample, we prefer to compute its expected value.
This quantity is known as the \textit{mean integrated squared error} or MISE.

\begin{align}
\mbox{MISE}(\hat{f}(.;h)) & = E \int{\{ \hat{f}(x;h) - f(x) \} ^2 }\,\mathrm{d}x && \nonumber \\
    & = \int{ E \{ \hat{f}(x;h) - f(x) \} ^2 }\,\mathrm{d}x & \text{by Fubini's Theorem} & \nonumber \\
    & = \int{ \mbox{MSE}(\hat{f}(x;h))\, \mathrm{d}x} & & \nonumber \\
    & = n^{-1} \int{ [ \conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x) ]\,\mathrm{d}x } \nonumber \\
    & \qquad + \int{ [ \conv{K_h}{f}(x) - f(x) ]^2\,\mathrm{d}x} & \text{by \autoref{eq:MSE}} \nonumber
\end{align}

Simplify it.

\begin{align}
\mbox{MISE}(\hat{f}(.;h)) & = n^{-1} \int{ [ \conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x) ]\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ [ \conv{K_h}{f}(x) - f(x) ]^2\,\mathrm{d}x} && \nonumber \\
    & = n^{-1} \int{ \conv{K_h^2}{f}(x)\,\mathrm{d}x } - n^{-1} \int{\conv{K_h}{f}^2(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ \conv{K_h}{f}^2(x)\,\mathrm{d}x} - 2\int{\conv{K_h}{f}(x) f(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ f^2(x)\,\mathrm{d}x} && \nonumber \\
    & = n^{-1} \int{ \conv{K_h^2}{f}(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + (1 - n^{-1}) \int{\conv{K_h}{f}^2(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad - 2\int{\conv{K_h}{f}(x) f(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ f^2(x)\,\mathrm{d}x} \label{eq:MISE-1} &&
\end{align}

Looking at the first term of \autoref{eq:MISE-1}, and using a change of variables $t = (x - y) / h$, so that $\mathrm{d}x = h \mathrm{d}t$

\begin{align}
n^{-1} \int{ \conv{K_h^2}{f}(x)\,\mathrm{d}x } & = n^{-1} \int{ \int {K_h^2(x-y)f(y)\,\mathrm{d}y} \,\mathrm{d}x } && \nonumber \\
    & = n^{-1} \int{ \int {h^{-2} K^2( (x-y)/h )f(y)\,\mathrm{d}y} \,\mathrm{d}x } && \nonumber \\
    & = n^{-1} \int{ \int {h^{-1} K^2(t)f(y)\,\mathrm{d}y} \,\mathrm{d}t } && \nonumber \\
    & = n^{-1} \int{ h^{-1} K^2(t) \int {f(y)\,\mathrm{d}y} \,\mathrm{d}t } && \nonumber \\
    & = (nh)^{-1} \int{ K^2(t) \,\mathrm{d}t } \label{eq:MISE-2} && \text{since f is a density}
\end{align}

Substituting \autoref{eq:MISE-2} in \autoref{eq:MISE-1}, gives the following formula for MISE:

\begin{align}
\mbox{MISE}(\hat{f}(.;h)) & = (nh)^{-1} \int{ K^2(x) \,\mathrm{d}x } && \nonumber \\
    & \qquad + (1 - n^{-1}) \int{\conv{K_h}{f}^2(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad - 2\int{\conv{K_h}{f}(x) f(x)\,\mathrm{d}x } && \nonumber \\
    & \qquad + \int{ f^2(x)\,\mathrm{d}x} \label{eq:MISE} &&
\end{align}

\subsection{Asymptotic Mean Integrated Squared Error}

The equation we derived for MISE, \autoref{eq:MISE}, depends on the bandwidth in such a way as to make it difficult to compute.
In this section we will derive an approximation, using Taylor's Theorem, that is far easier to compute.

There are 3 assumptions that we make, as described in (\citep[pp. 19-20]{wand1994kernel}:

\begin{enumerate}
    \item \label{asm:second_deriv} The true density $f$ has a second derivative $f^{\prime\prime}$ that is continous, square integrable, and ultimately monotone.
    \item \label{asm:h_to_zero} There is a sequence of bandwidths, \mbox{$\{h_n : 1 < n < \infty\}$} such that each \mbox{$h_n > 0$, $\lim_{n \to \infty} h_n = 0$} and \mbox{$\lim_{n \to \infty} nh = \infty$}.
    \item \label{asm:K_pdf} $K$ is a bounded probability density with a finite fourth moment and symmetry about the origin.
\end{enumerate}

\subsubsection{Asymptotic Bias}

Starting with \autoref{eq:E_f_hat}, we see:

\begin{align}
E\hat{f}(x;h) & = \int{K_h(x - y)f(y)\,\mathrm{d}y} && \nonumber \\
    & = \int{h^{-1}K(x-y)f(y)\,\mathrm{d}y} && \nonumber \\
    & = \int{K(z)f(x-hz)\,\mathrm{d}z} && \text{substituting } z=y/h
\end{align}

By Taylor's Theorem:

\begin{align}
f(x-hz) & = f(x) - hzf^\prime(x) + \sfrac{1}{2}h^2z^2f^{\prime\prime}(x) + R_2(h) &&
\end{align}
where
\begin{align}
\lim_{h \to 0} \dfrac{R_2(h)}{h^2} = 0 && \nonumber
\end{align}

Substituting, we get:
\begin{align}
E\hat{f}(x;h) & = \int{K(z) \left[ f(x) - hzf^\prime(x) + \sfrac{1}{2}h^2z^2f^{\prime\prime}(x) + R_2(h) \right] \,\mathrm{d}z} && \nonumber \\
    & = f(x)\int{K(z)\mathrm{d}z} - hf^\prime(x)\int{K(z)\mathrm{d}z} + \sfrac{1}{2}h^2f^{\prime\prime}(x)\int{z^2K(z)\mathrm{d}z} + R_2(h)\int{K(z)\mathrm{d}z} && \nonumber
\end{align}

Since $K$ is a pdf, $\int{K(z)\,\mathrm{d}z}=1$.
Since $K$ is symmetric about the origin, $\int{zK(z)\,\mathrm{d}z}=0$.
By our assumption \ref{asm:K_pdf}, $\int{z^2K(z)\,\mathrm{d}z} < \infty$.
Therefore,

\begin{align}
E\hat{f}(x;h) & = f(x) + \sfrac{1}{2}h^2f^{\prime\prime}(x)\int{z^2K(z)\,\mathrm{d}z} + R_2(h) && \nonumber
\end{align}

This leads to the following approximation of the bias:

\begin{align}
\mbox{Bias}(f(x;h)) & = \sfrac{1}{2}h^2f^{\prime\prime}(x)\int{z^2K(z)\,\mathrm{d}z} + R_2(h) && \label{eq:ABias}
\end{align}

\subsubsection{Asymptotic Variance}

Let's start with \autoref{eq:MSE},

\begin{align}
\mbox{Var}(f(x;h)) & = n^{-1} \int{\left[\conv{K_h^2}{f}(x) - \conv{K_h}{f}^2(x)\right]\mathrm{d}x} && \nonumber \\
    & = (nh)^{-1} \int{K(z)^2f(x - hz)\,\mathrm{d}z} - n^{-1} [E(\hat{f}(x;h))]^2 && \nonumber \\
    & = (nh)^{-1} \int{K(z)^2[f(x) + o(1)]\,\mathrm{d}z} - n^{-1}[f(x) + o(1)]^2 && \nonumber \\
    & = (nh)^{-1} \int{K(z)^2f(x)\mathrm{d}z} + \int{K(z)^2o(1)\,\mathrm{d}z} + o(n^{-2})  && \nonumber \\
    & = (nh)^{-1} \int{K(z)^2f(x)\,\mathrm{d}z} + o((nh)^{-1}) + o((nh)^{-1}) && \nonumber \\
    & = (nh)^{-1} \int{K(z)^2f(x)\,\mathrm{d}z} + o((nh)^{-1}) && \label{eq:AVar}
\end{align}

Adding the approximations of the square of the bias from \autoref{eq:ABias} and the variance from \autoref{eq:AVar}, we get:

\begin{align}
\mbox{MSE}(\hat{f}(x;h)) & = (nh)^{-1} \int{K^2}f(x) + \sfrac{1}{4}h^4\left(\int{z^2K}\right)^2f^{\prime\prime}(x)^2 + o((nh)^{-1} + h^4) && \label{eq:EMSE}
\end{align}

The mean integrated squared error is obtained by integrating \autoref{eq:EMSE}:

\begin{align}
\mbox{MISE}(\hat{f}) & = (nh)^{-1} \int{K^2}\int{f} + \sfrac{1}{4}h^4\left(\int{z^2K}\right)^2 \int{ (f^{\prime\prime})^2 } + o((nh)^{-1} + h^4) && \label{eq:EMISE}
\end{align}

We then obtain the formula for asymptotic mean integrated squared error (AMISE) as:

\begin{align}
\mbox{AMISE}(\hat{f}) & = (nh)^{-1} \int{K^2}\int{f} + \sfrac{1}{4}h^4\left(\int{z^2K}\right)^2 \int{ (f^{\prime\prime})^2 } && \label{eq:AMISE}
\end{align}



